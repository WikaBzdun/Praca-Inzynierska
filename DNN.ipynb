{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "\n",
    "# Załaduj dane IMDB\n",
    "(train_data, test_data), info = tfds.load(\n",
    "    \"imdb_reviews\",\n",
    "    split=(\"train\", \"test\"),\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "# Przetwórz dane tekstowe na indeksy słów\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "MAX_TOKENS = 10000  # Maksymalna liczba słów w słowniku\n",
    "MAX_SEQ_LEN = 256   # Maksymalna długość sekwencji\n",
    "\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS, output_sequence_length=MAX_SEQ_LEN\n",
    ")\n",
    "\n",
    "# Dopasuj encoder na danych uczących\n",
    "train_text = train_data.map(lambda text, label: text)\n",
    "encoder.adapt(train_text)\n",
    "\n",
    "# Funkcja do przetwarzania danych\n",
    "def preprocess(text, label):\n",
    "    return encoder(text), label\n",
    "\n",
    "train_data = train_data.map(preprocess).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataMAP = test_data.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "# Budowa modelu\n",
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=MAX_TOKENS, output_dim=128, mask_zero=True),\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
    "    layers.Bidirectional(layers.LSTM(32)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Kompilacja modelu\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Wyświetl podsumowanie modelu\n",
    "model.summary()\n",
    "\n",
    "# Trenowanie modelu\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=test_dataMAP,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Ocena modelu\n",
    "results = model.evaluate(test_dataMAP)\n",
    "print(f\"Test Loss: {results[0]:.4f}, Test Accuracy: {results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wyświetlenie danych \n",
    "(train_data, test_data), info = tfds.load(\n",
    "    \"imdb_reviews\",\n",
    "    split=(\"train\", \"test\"),\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "train_df = pd.DataFrame([\n",
    "    {\"Review\": text.numpy().decode('utf-8'), \"Label\": \"Positive\" if label.numpy() == 1 else \"Negative\"}\n",
    "    for text, label in train_data.take(50) \n",
    "])\n",
    "#train_df.style.set_properties(**{'white-space': 'pre-wrap'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Optymalizacja \n",
    "# Dostosowanie parametrów\n",
    "BUFFER_SIZE = 10000  \n",
    "BATCH_SIZE = 64      \n",
    "MAX_TOKENS = 10000   \n",
    "MAX_SEQ_LEN = 256   \n",
    "\n",
    "\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS, output_sequence_length=MAX_SEQ_LEN\n",
    ")\n",
    "\n",
    "# Dopasuj encoder na danych uczących\n",
    "train_text = train_data.map(lambda text, label: text)\n",
    "encoder.adapt(train_text)\n",
    "\n",
    "# Funkcja do przetwarzania danych\n",
    "def preprocess(text, label):\n",
    "    return encoder(text), label\n",
    "\n",
    "train_data = train_data.map(preprocess).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataMAP = test_data.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#Budowa modelu\n",
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=MAX_TOKENS, output_dim=128, mask_zero=True, input_length=MAX_SEQ_LEN),\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)),\n",
    "    layers.Bidirectional(layers.LSTM(32, dropout=0.4, recurrent_dropout=0.4)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Kompilacja modelu\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Podsumowanie modelu\n",
    "model.build(input_shape=(None, MAX_SEQ_LEN))\n",
    "model.summary()\n",
    "\n",
    "#Konfiguracja EarlyStopping \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Trenowanie modelu\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=test_dataMAP,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Ocena modelu\n",
    "results = model.evaluate(test_dataMAP)\n",
    "print(f\"Test Loss: {results[0]:.4f}, Test Accuracy: {results[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Rozpakowanie oryginalnych danych testowych\n",
    "original_texts = []\n",
    "true_labels = []\n",
    "for text, label in test_data:  \n",
    "    original_texts.append(text.numpy().decode('utf-8')) \n",
    "    true_labels.append(label.numpy())  \n",
    "\n",
    "processed_texts = []\n",
    "processed_labels = []\n",
    "for text_vector, label in test_dataMAP.unbatch():\n",
    "    processed_texts.append(text_vector.numpy()) \n",
    "    processed_labels.append(label.numpy())  \n",
    "\n",
    "predicted_probs = model.predict(test_dataMAP)\n",
    "predicted_labels = (predicted_probs > 0.5).astype(\"int32\").flatten() \n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Original Text': original_texts,\n",
    "    'Processed Text': processed_texts,\n",
    "    'True Label': true_labels,\n",
    "    'Predicted Label': predicted_labels\n",
    "})\n",
    "\n",
    "print(df.head())\n",
    "df.to_csv(\"plik.csv\", index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 300)\n",
    "row_as_table = df.loc[[15339], ['Original Text', 'True Label', 'Predicted Label']]\n",
    "row_as_table\n",
    "row_as_table.style.set_properties(**{'white-space': 'pre-wrap'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Wykres macierzy \n",
    "def plot_confusion_matrix(cm):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Wykresy strat i dokładności\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Strata podczas nauki')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Dokładność podczas nauki')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
